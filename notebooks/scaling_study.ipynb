{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileLLM Scaling Laws Study\n",
    "\n",
    "Reproducible assessment of MobileLLM family (125M to 1B) to analyze scaling behavior in sub-billion parameter regime.\n",
    "\n",
    "**Run on Google Colab with GPU runtime (T4 recommended)**\n",
    "\n",
    "## What this notebook does:\n",
    "1. Loads MobileLLM models (125M, 350M, 600M, 1B)\n",
    "2. Computes perplexity on WikiText-2\n",
    "3. Runs downstream tasks via lm-evaluation-harness\n",
    "4. Tracks system metrics (tokens/sec, memory, wall-clock)\n",
    "5. Fits scaling curves and generates plots\n",
    "6. Exports results to JSON/CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q transformers datasets accelerate sentencepiece lm-eval torch\n",
    "!pip install -q scipy scikit-learn matplotlib seaborn pandas tqdm psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with HuggingFace (required for gated MobileLLM models)\n",
    "from huggingface_hub import login\n",
    "login()  # Enter your HF token when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = Path('results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileLLM model configurations\n",
    "MODELS = {\n",
    "    \"MobileLLM-125M\": {\n",
    "        \"hf_name\": \"facebook/MobileLLM-125M\",\n",
    "        \"params\": 124.6e6,\n",
    "        \"layers\": 30,\n",
    "        \"hidden\": 576,\n",
    "    },\n",
    "    \"MobileLLM-350M\": {\n",
    "        \"hf_name\": \"facebook/MobileLLM-350M\",\n",
    "        \"params\": 345.3e6,\n",
    "        \"layers\": 32,\n",
    "        \"hidden\": 960,\n",
    "    },\n",
    "    \"MobileLLM-600M\": {\n",
    "        \"hf_name\": \"facebook/MobileLLM-600M\",\n",
    "        \"params\": 603.1e6,\n",
    "        \"layers\": 40,\n",
    "        \"hidden\": 1152,\n",
    "    },\n",
    "    \"MobileLLM-1B\": {\n",
    "        \"hf_name\": \"facebook/MobileLLM-1B\",\n",
    "        \"params\": 1.01e9,\n",
    "        \"layers\": 54,\n",
    "        \"hidden\": 1280,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Downstream tasks (matching MobileLLM paper)\n",
    "DOWNSTREAM_TASKS = [\n",
    "    \"arc_easy\",\n",
    "    \"arc_challenge\",\n",
    "    \"boolq\",\n",
    "    \"piqa\",\n",
    "    \"hellaswag\",\n",
    "    \"winogrande\",\n",
    "    \"openbookqa\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AssessmentResults:\n",
    "    \"\"\"Container for assessment results.\"\"\"\n",
    "    model_name: str\n",
    "    num_params: float\n",
    "    perplexity: float\n",
    "    downstream_scores: Dict[str, float]\n",
    "    tokens_per_second: float\n",
    "    peak_memory_mb: float\n",
    "    wall_clock_seconds: float\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perplexity (Sliding Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_sliding_window(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset_name: str = \"wikitext\",\n",
    "    dataset_config: str = \"wikitext-2-raw-v1\",\n",
    "    split: str = \"test\",\n",
    "    max_length: int = 1024,\n",
    "    stride: int = 512,\n",
    "    device: str = \"cuda\",\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compute perplexity using HuggingFace's recommended sliding window approach.\n",
    "    \n",
    "    This avoids edge artifacts by using overlapping windows and only\n",
    "    computing NLL on the non-overlapping portion.\n",
    "    \n",
    "    Reference: https://huggingface.co/docs/transformers/perplexity\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, dataset_config, split=split)\n",
    "    \n",
    "    # Concatenate all text\n",
    "    text = \"\\n\\n\".join(dataset[\"text\"])\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    print(f\"Dataset tokens: {seq_len:,}\")\n",
    "    \n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Computing perplexity\"):\n",
    "            end_loc = min(begin_loc + max_length, seq_len)\n",
    "            trg_len = end_loc - prev_end_loc  # Only score non-overlapping portion\n",
    "            \n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "            target_ids = input_ids.clone()\n",
    "            \n",
    "            # Mask tokens we've already scored\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "            \n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "            nlls.append(neg_log_likelihood)\n",
    "            \n",
    "            prev_end_loc = end_loc\n",
    "            if end_loc == seq_len:\n",
    "                break\n",
    "    \n",
    "    wall_time = time.time() - start_time\n",
    "    total_nll = torch.stack(nlls).sum()\n",
    "    perplexity = torch.exp(total_nll / seq_len).item()\n",
    "    \n",
    "    return {\n",
    "        \"perplexity\": perplexity,\n",
    "        \"total_tokens\": seq_len,\n",
    "        \"wall_clock_seconds\": wall_time,\n",
    "        \"tokens_per_second\": seq_len / wall_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. System Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_mb() -> float:\n",
    "    \"\"\"Get current GPU memory usage in MB.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.max_memory_allocated() / 1e6\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def reset_memory_stats():\n",
    "    \"\"\"Reset GPU memory stats.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def measure_inference_speed(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str = \"The quick brown fox\",\n",
    "    num_tokens: int = 50,\n",
    "    num_runs: int = 5,\n",
    "    device: str = \"cuda\",\n",
    ") -> Dict:\n",
    "    \"\"\"Measure inference speed in tokens/second.\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_new_tokens=10, use_cache=False)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=num_tokens,\n",
    "                do_sample=False,\n",
    "                use_cache=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    tokens_per_second = num_tokens / avg_time\n",
    "    \n",
    "    return {\n",
    "        \"tokens_per_second\": tokens_per_second,\n",
    "        \"avg_time_seconds\": avg_time,\n",
    "        \"std_time_seconds\": np.std(times),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Downstream Tasks (lm-harness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lm_harness_tasks(\n",
    "    model_name: str,\n",
    "    tasks: List[str],\n",
    "    batch_size: int = 8,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Run lm-harness using Python API directly.\n",
    "    \"\"\"\n",
    "    from lm_eval import evaluator\n",
    "    from lm_eval.models.huggingface import HFLM\n",
    "    \n",
    "    # Create model wrapper\n",
    "    lm = HFLM(\n",
    "        pretrained=model_name,\n",
    "        trust_remote_code=True,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    # Run assessment\n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=lm,\n",
    "        tasks=tasks,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    # Extract scores\n",
    "    scores = {}\n",
    "    for task, metrics in results.get(\"results\", {}).items():\n",
    "        acc = metrics.get(\"acc,none\") or metrics.get(\"acc_norm,none\")\n",
    "        if acc is not None:\n",
    "            scores[task] = acc\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Full Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model_name: str, model_config: Dict, device: str = \"cuda\") -> AssessmentResults:\n",
    "    \"\"\"\n",
    "    Run full assessment suite on a single model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Assessing: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    hf_name = model_config[\"hf_name\"]\n",
    "    start_time = time.time()\n",
    "    reset_memory_stats()\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_name, use_fast=False)\n",
    "    tokenizer.add_special_tokens({\"eos_token\": \"</s>\", \"bos_token\": \"<s>\", \"unk_token\": \"<unk>\"})\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        hf_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    actual_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {actual_params:,}\")\n",
    "    \n",
    "    # 1. Perplexity\n",
    "    print(\"\\nComputing perplexity...\")\n",
    "    ppl_results = compute_perplexity_sliding_window(\n",
    "        model, tokenizer, device=device\n",
    "    )\n",
    "    print(f\"Perplexity: {ppl_results['perplexity']:.2f}\")\n",
    "    \n",
    "    # 2. Inference speed\n",
    "    print(\"\\nMeasuring inference speed...\")\n",
    "    speed_results = measure_inference_speed(model, tokenizer, device=device)\n",
    "    print(f\"Tokens/sec: {speed_results['tokens_per_second']:.1f}\")\n",
    "    \n",
    "    # 3. Downstream tasks\n",
    "    print(\"\\nRunning downstream tasks...\")\n",
    "    downstream_scores = run_lm_harness_tasks(hf_name, DOWNSTREAM_TASKS)\n",
    "    \n",
    "    for task, score in downstream_scores.items():\n",
    "        print(f\"  {task}: {score*100:.1f}%\")\n",
    "    \n",
    "    # Collect results\n",
    "    wall_clock = time.time() - start_time\n",
    "    peak_memory = get_gpu_memory_mb()\n",
    "    \n",
    "    results = AssessmentResults(\n",
    "        model_name=model_name,\n",
    "        num_params=actual_params,\n",
    "        perplexity=ppl_results[\"perplexity\"],\n",
    "        downstream_scores=downstream_scores,\n",
    "        tokens_per_second=speed_results[\"tokens_per_second\"],\n",
    "        peak_memory_mb=peak_memory,\n",
    "        wall_clock_seconds=wall_clock,\n",
    "    )\n",
    "    \n",
    "    # Save individual model results\n",
    "    model_dir = RESULTS_DIR / model_name.lower().replace(\"-\", \"_\")\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(model_dir / \"results.json\", \"w\") as f:\n",
    "        json.dump(results.to_dict(), f, indent=2)\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run assessment on all models\n",
    "all_results = []\n",
    "\n",
    "for model_name, model_config in MODELS.items():\n",
    "    try:\n",
    "        results = assess_model(model_name, model_config)\n",
    "        all_results.append(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to assess {model_name}: {e}\")\n",
    "\n",
    "print(f\"\\nCompleted {len(all_results)}/{len(MODELS)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scaling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame([r.to_dict() for r in all_results])\n",
    "\n",
    "# Expand downstream scores\n",
    "downstream_df = pd.json_normalize(df[\"downstream_scores\"])\n",
    "df = pd.concat([df.drop(columns=[\"downstream_scores\"]), downstream_df], axis=1)\n",
    "\n",
    "# Calculate average downstream accuracy\n",
    "task_cols = [c for c in df.columns if c in DOWNSTREAM_TASKS]\n",
    "df[\"avg_accuracy\"] = df[task_cols].mean(axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_scaling_law(x, y, name=\"\"):\n",
    "    \"\"\"\n",
    "    Fit log-linear scaling law: log(y) = alpha * log(x) + beta\n",
    "    Returns slope (alpha), intercept (beta), R-squared, and confidence intervals.\n",
    "    \"\"\"\n",
    "    log_x = np.log10(x)\n",
    "    log_y = np.log10(y)\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(log_x, log_y)\n",
    "    \n",
    "    print(f\"\\n{name} Scaling Law:\")\n",
    "    print(f\"  y = {10**intercept:.4f} * x^{slope:.4f}\")\n",
    "    print(f\"  R^2 = {r_value**2:.4f}\")\n",
    "    print(f\"  Slope SE = {std_err:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"slope\": slope,\n",
    "        \"intercept\": intercept,\n",
    "        \"r_squared\": r_value**2,\n",
    "        \"std_err\": std_err,\n",
    "        \"p_value\": p_value,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaling laws\n",
    "params = df[\"num_params\"].values\n",
    "\n",
    "# Perplexity scaling (expect negative slope - larger models = lower perplexity)\n",
    "ppl_fit = fit_scaling_law(params, df[\"perplexity\"].values, \"Perplexity\")\n",
    "\n",
    "# Accuracy scaling (expect positive slope - larger models = higher accuracy)\n",
    "acc_fit = fit_scaling_law(params, df[\"avg_accuracy\"].values, \"Average Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Perplexity vs Parameters\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(df[\"num_params\"], df[\"perplexity\"], s=100, zorder=5)\n",
    "for i, row in df.iterrows():\n",
    "    ax1.annotate(row[\"model_name\"], (row[\"num_params\"], row[\"perplexity\"]),\n",
    "                 xytext=(5, 5), textcoords=\"offset points\", fontsize=8)\n",
    "\n",
    "# Fit line\n",
    "x_fit = np.logspace(np.log10(params.min()), np.log10(params.max()), 100)\n",
    "y_fit = 10**ppl_fit[\"intercept\"] * x_fit**ppl_fit[\"slope\"]\n",
    "ax1.plot(x_fit, y_fit, 'r--', alpha=0.7, label=f\"Fit: a={ppl_fit['slope']:.3f}\")\n",
    "\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.set_xlabel(\"Parameters\")\n",
    "ax1.set_ylabel(\"Perplexity (WikiText-2)\")\n",
    "ax1.set_title(\"Perplexity Scaling\")\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Accuracy vs Parameters\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(df[\"num_params\"], df[\"avg_accuracy\"] * 100, s=100, zorder=5)\n",
    "for i, row in df.iterrows():\n",
    "    ax2.annotate(row[\"model_name\"], (row[\"num_params\"], row[\"avg_accuracy\"] * 100),\n",
    "                 xytext=(5, 5), textcoords=\"offset points\", fontsize=8)\n",
    "\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Parameters\")\n",
    "ax2.set_ylabel(\"Average Accuracy (%)\")\n",
    "ax2.set_title(\"Downstream Task Scaling\")\n",
    "\n",
    "# Plot 3: Per-task breakdown\n",
    "ax3 = axes[1, 0]\n",
    "task_data = df.melt(\n",
    "    id_vars=[\"model_name\", \"num_params\"],\n",
    "    value_vars=task_cols,\n",
    "    var_name=\"task\",\n",
    "    value_name=\"accuracy\"\n",
    ")\n",
    "sns.barplot(data=task_data, x=\"task\", y=\"accuracy\", hue=\"model_name\", ax=ax3)\n",
    "ax3.set_ylabel(\"Accuracy\")\n",
    "ax3.set_title(\"Per-Task Performance\")\n",
    "ax3.tick_params(axis=\"x\", rotation=45)\n",
    "ax3.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Plot 4: Efficiency (accuracy per billion params)\n",
    "ax4 = axes[1, 1]\n",
    "df[\"efficiency\"] = df[\"avg_accuracy\"] / (df[\"num_params\"] / 1e9)\n",
    "bars = ax4.bar(df[\"model_name\"], df[\"efficiency\"])\n",
    "ax4.set_ylabel(\"Accuracy per Billion Params\")\n",
    "ax4.set_title(\"Parameter Efficiency\")\n",
    "ax4.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"scaling_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined results\n",
    "df.to_csv(RESULTS_DIR / \"scaling_results.csv\", index=False)\n",
    "\n",
    "# Save scaling law fits\n",
    "scaling_fits = {\n",
    "    \"perplexity\": ppl_fit,\n",
    "    \"accuracy\": acc_fit,\n",
    "}\n",
    "with open(RESULTS_DIR / \"scaling_fits.json\", \"w\") as f:\n",
    "    json.dump(scaling_fits, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {RESULTS_DIR}/\")\n",
    "print(f\"  - scaling_results.csv\")\n",
    "print(f\"  - scaling_fits.json\")\n",
    "print(f\"  - scaling_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report\n",
    "report = f\"\"\"# MobileLLM Scaling Laws Report\n",
    "\n",
    "## Summary\n",
    "\n",
    "Assessed {len(all_results)} MobileLLM models (125M to 1B parameters) on:\n",
    "- WikiText-2 perplexity\n",
    "- {len(DOWNSTREAM_TASKS)} downstream tasks\n",
    "\n",
    "## Scaling Law Fits\n",
    "\n",
    "### Perplexity\n",
    "- **Formula:** PPL = {10**ppl_fit['intercept']:.2f} x N^{ppl_fit['slope']:.3f}\n",
    "- **R^2:** {ppl_fit['r_squared']:.4f}\n",
    "\n",
    "### Average Accuracy\n",
    "- **Formula:** Acc = {10**acc_fit['intercept']:.4f} x N^{acc_fit['slope']:.3f}\n",
    "- **R^2:** {acc_fit['r_squared']:.4f}\n",
    "\n",
    "## Model Results\n",
    "\n",
    "| Model | Params | Perplexity | Avg Acc | Tokens/s | Memory (MB) |\n",
    "|-------|--------|------------|---------|----------|-------------|\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    report += f\"| {row['model_name']} | {row['num_params']/1e6:.0f}M | {row['perplexity']:.2f} | {row['avg_accuracy']*100:.1f}% | {row['tokens_per_second']:.0f} | {row['peak_memory_mb']:.0f} |\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "## Files\n",
    "\n",
    "- `scaling_results.csv` - Raw results\n",
    "- `scaling_fits.json` - Fitted scaling law parameters\n",
    "- `scaling_curves.png` - Visualization\n",
    "\"\"\"\n",
    "\n",
    "with open(RESULTS_DIR / \"report.md\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Report saved to results/report.md\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
