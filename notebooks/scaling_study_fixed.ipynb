{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileLLM Scaling Laws Study\n",
    "\n",
    "**Run on Google Colab with T4 GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate sentencepiece\n",
    "!pip install -q scipy scikit-learn matplotlib seaborn pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"No GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "RESULTS_DIR = Path('results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"MobileLLM-125M\": {\"hf_name\": \"facebook/MobileLLM-125M\", \"params\": 124.6e6},\n",
    "    \"MobileLLM-350M\": {\"hf_name\": \"facebook/MobileLLM-350M\", \"params\": 345.3e6},\n",
    "    \"MobileLLM-600M\": {\"hf_name\": \"facebook/MobileLLM-600M\", \"params\": 603.1e6},\n",
    "    \"MobileLLM-1B\": {\"hf_name\": \"facebook/MobileLLM-1B\", \"params\": 1.01e9},\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class ModelResults:\n",
    "    model_name: str\n",
    "    num_params: int\n",
    "    perplexity: float\n",
    "    tokens_per_second: float\n",
    "    peak_memory_mb: float\n",
    "    wall_clock_seconds: float\n",
    "    def to_dict(self): return asdict(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, tokenizer, max_length=1024, stride=512, device=\"cuda\"):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    text = \"\\n\\n\".join(dataset[\"text\"])\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    print(f\"Tokens: {seq_len:,}\")\n",
    "    \n",
    "    nlls, prev_end = [], 0\n",
    "    model.eval()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for begin in tqdm(range(0, seq_len, stride), desc=\"Perplexity\"):\n",
    "            end = min(begin + max_length, seq_len)\n",
    "            trg_len = end - prev_end\n",
    "            ids = encodings.input_ids[:, begin:end].to(device)\n",
    "            tgt = ids.clone()\n",
    "            tgt[:, :-trg_len] = -100\n",
    "            out = model(ids, labels=tgt)\n",
    "            nlls.append(out.loss * trg_len)\n",
    "            prev_end = end\n",
    "            if end == seq_len: break\n",
    "    \n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / seq_len).item()\n",
    "    return {\"perplexity\": ppl, \"tokens\": seq_len, \"time\": time.time() - t0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_speed(model, tokenizer, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\"The quick brown fox\", return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad(): model.generate(**inputs, max_new_tokens=10, use_cache=False)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            model.generate(**inputs, max_new_tokens=50, do_sample=False, use_cache=False, pad_token_id=tokenizer.eos_token_id)\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - t0)\n",
    "    return 50 / np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(name, config):\n",
    "    print(f\"\\n{'='*50}\\n{name}\\n{'='*50}\")\n",
    "    t0 = time.time()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    hf = config[\"hf_name\"]\n",
    "    tok = AutoTokenizer.from_pretrained(hf, use_fast=False)\n",
    "    tok.add_special_tokens({\"eos_token\": \"</s>\", \"bos_token\": \"<s>\", \"unk_token\": \"<unk>\"})\n",
    "    model = AutoModelForCausalLM.from_pretrained(hf, trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    \n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Params: {params:,}\")\n",
    "    \n",
    "    ppl = compute_perplexity(model, tok)\n",
    "    print(f\"PPL: {ppl['perplexity']:.2f}\")\n",
    "    \n",
    "    speed = measure_speed(model, tok)\n",
    "    print(f\"Speed: {speed:.1f} tok/s\")\n",
    "    \n",
    "    mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "    result = ModelResults(name, params, ppl[\"perplexity\"], speed, mem, time.time() - t0)\n",
    "    \n",
    "    del model, tok\n",
    "    torch.cuda.empty_cache()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for name, cfg in MODELS.items():\n",
    "    try:\n",
    "        all_results.append(run_model(name, cfg))\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED {name}: {e}\")\n",
    "print(f\"\\nDone: {len(all_results)}/{len(MODELS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([r.to_dict() for r in all_results])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaling law\n",
    "log_n = np.log10(df[\"num_params\"])\n",
    "log_ppl = np.log10(df[\"perplexity\"])\n",
    "slope, intercept, r, p, se = stats.linregress(log_n, log_ppl)\n",
    "\n",
    "print(f\"\\nSCALING LAW: PPL = {10**intercept:.0f} * N^({slope:.4f})\")\n",
    "print(f\"R-squared = {r**2:.4f}\")\n",
    "\n",
    "ppl_fit = {\"slope\": slope, \"intercept\": intercept, \"r_squared\": r**2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perplexity scaling\n",
    "ax1.scatter(df[\"num_params\"], df[\"perplexity\"], s=150, c='steelblue', edgecolors='black', zorder=5)\n",
    "for _, row in df.iterrows():\n",
    "    ax1.annotate(row[\"model_name\"].replace(\"MobileLLM-\",\"\"), \n",
    "                 (row[\"num_params\"], row[\"perplexity\"]), xytext=(8,0), \n",
    "                 textcoords=\"offset points\", fontsize=11, fontweight='bold')\n",
    "\n",
    "x_fit = np.logspace(np.log10(df[\"num_params\"].min()*0.7), np.log10(df[\"num_params\"].max()*1.3), 100)\n",
    "y_fit = 10**intercept * x_fit**slope\n",
    "ax1.plot(x_fit, y_fit, 'r--', lw=2, label=f\"PPL = {10**intercept:.0f} x N^({slope:.3f})\\nR² = {r**2:.4f}\")\n",
    "\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.set_xlabel(\"Parameters\", fontsize=12)\n",
    "ax1.set_ylabel(\"Perplexity\", fontsize=12)\n",
    "ax1.set_title(\"MobileLLM Perplexity Scaling\", fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Speed\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(df)))\n",
    "bars = ax2.bar(df[\"model_name\"].str.replace(\"MobileLLM-\",\"\"), df[\"tokens_per_second\"], color=colors, edgecolor='black')\n",
    "for bar, val in zip(bars, df[\"tokens_per_second\"]):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.5, f'{val:.1f}', ha='center', fontweight='bold')\n",
    "ax2.set_ylabel(\"Tokens/sec\", fontsize=12)\n",
    "ax2.set_xlabel(\"Model\", fontsize=12)\n",
    "ax2.set_title(\"Inference Speed (T4)\", fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"scaling.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(RESULTS_DIR / \"results.csv\", index=False)\n",
    "with open(RESULTS_DIR / \"scaling_fit.json\", \"w\") as f:\n",
    "    json.dump(ppl_fit, f, indent=2)\n",
    "print(\"Saved to results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = f\"\"\"# MobileLLM Scaling Laws Report\n",
    "\n",
    "## Scaling Law\n",
    "```\n",
    "PPL = {10**intercept:.0f} x N^({slope:.4f})\n",
    "R² = {r**2:.4f}\n",
    "```\n",
    "\n",
    "## Results\n",
    "| Model | Params | PPL | Tok/s | Memory |\n",
    "|-------|--------|-----|-------|--------|\n",
    "\"\"\"\n",
    "for _, row in df.iterrows():\n",
    "    report += f\"| {row['model_name']} | {row['num_params']/1e6:.0f}M | {row['perplexity']:.2f} | {row['tokens_per_second']:.1f} | {row['peak_memory_mb']/1000:.1f}GB |\\n\"\n",
    "\n",
    "with open(RESULTS_DIR / \"report.md\", \"w\") as f:\n",
    "    f.write(report)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(f\"RESULT: PPL = {10**intercept:.0f} x N^({slope:.4f})\")\n",
    "print(f\"R² = {r**2:.4f}\")\n",
    "print(f\"2x params -> {100*(1-2**slope):.1f}% lower perplexity\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "T4"},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
