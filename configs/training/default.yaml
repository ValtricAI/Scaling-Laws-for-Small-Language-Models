# Default Training Configuration

training:
  # Batch settings
  batch_size: 32
  gradient_accumulation_steps: 4
  max_seq_length: 2048

  # Optimizer settings
  optimizer: "adamw"
  learning_rate: 3.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # Schedule
  scheduler: "cosine"
  warmup_steps: 1000
  max_steps: 100000
  min_lr_ratio: 0.1

  # Mixed precision
  fp16: false
  bf16: true

  # Checkpointing
  save_steps: 5000
  eval_steps: 1000
  logging_steps: 100
  save_total_limit: 3

  # Reproducibility
  seed: 42

data:
  dataset: "openwebtext"
  num_workers: 4
  prefetch_factor: 2

logging:
  wandb:
    enabled: true
    project: "scaling-laws-slm"
    entity: null
    tags: ["mobilellm", "scaling"]
